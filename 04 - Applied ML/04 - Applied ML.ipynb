{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd             \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt   \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats.mstats import mode\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will do some data preprocessing to make the dataset ready for training. Indeed by reading through the example notebook and by further analysis. Some inconsitencies will be problematic durint the training of the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('CrowdstormingDataJuly1st.csv', index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to remove all the rows from which we can not infere the answer to the question asked. We need to find the skin color of the soccer player based on the other feature at our disposal. We will thus first take interest in the column '_rater1_' and '_rater2_'. By looking at the dataset we can directly see that some rows have no \"rating\" and won't be able to help use during the training: we don't have the output labels for the classifier. This is only a real problem when using supervised learning.\n",
    "\n",
    "We only have rating for ~85% of the dataset also each sample is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Good news we always have either the two rating or none\n",
    "sum(~(df.rater1.isnull() == df.rater2.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = df.dropna(subset=['rater1', 'rater2']).copy()\n",
    "print('Total available', len(df))\n",
    "print('Total with rating', len(data), \"({}%)\".format(round(len(data)*100/len(df),3)))\n",
    "print('Number of sample with disagrement:', sum(~(data.rater1 == data.rater2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['skin'] = data[['rater1', 'rater2']].max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at own many \"real\" unique sample we have. Indeed a player is most certainly present multiple times. As we can see most of the players have several entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique player\", len(data.player.unique()))\n",
    "rows_per_player = data.player.value_counts()\n",
    "ax = rows_per_player.hist(bins=28, range=(0, 280))\n",
    "ax.set_xlabel('Number of apparition')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will aggregate the data to only work with a sample per player. Some of the player are present only one time and other more than one hundred time.\n",
    "\n",
    "Also a classifier as no notions of strings as input we thus need to deal with the columns with text features and encode them in a different way. We could either use a numbering encoding (clubX = 1, clubY = 2, etc) or juste dummy encode the column. We will use the dummy encoding.\n",
    "\n",
    "We also deal with the missing data (_nan_ values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill Nan values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the missing values. We will have to deal with all of them before feeding anything to the classifier. As we can see some of the column have several missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Height & Weight : Use mean\n",
    "data['height'] = data['height'].fillna(data['height'].mean())\n",
    "data['weight'] = data['weight'].fillna(data['weight'].mean())\n",
    "\n",
    "# Position: add mew label\n",
    "data['position'] = data['position'].fillna('UNKNOWN')\n",
    "\n",
    "# We decided to drop all rows containging nan in the rest of the columns. Indeed it would be difficult to \n",
    "# decide by which value we need to fill the data as they are specific for each dyad player - referee\n",
    "data.dropna(subset=['Alpha_3', 'meanIAT', 'nIAT','seIAT','meanExp','nExp','seExp'],axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to work only with information about player not dyad player-referee. We need to aggregate the unformation about each player into a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO Add age\n",
    "#len(data['club'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_present = lambda x: x.value_counts().index[0]\n",
    "\n",
    "players = data.groupby(level=0).agg({\n",
    "    'leagueCountry': most_present,\n",
    "    'position': most_present,\n",
    "    'club': most_present,\n",
    "    \n",
    "    'height': 'mean', \n",
    "    'weight': 'mean',\n",
    "\n",
    "    'meanIAT': most_present,\n",
    "    'meanExp':'mean', \n",
    "    #'seIAT':'mean', \n",
    "    #'seExp':'mean',\n",
    "\n",
    "    'games':'sum', \n",
    "\n",
    "    'victories':'sum',\n",
    "    'defeats':'sum', \n",
    "    'ties': 'sum', \n",
    "\n",
    "    'goals':'mean', \n",
    "\n",
    "    'redCards':'mean', \n",
    "    'yellowReds': 'mean', \n",
    "    'yellowCards':'mean',\n",
    "\n",
    "    'skin': most_present,\n",
    "})\n",
    "\n",
    "players = data.groupby(level=0).agg({\n",
    "        'leagueCountry': 'first',\n",
    "        'position': 'first',\n",
    "        'height':'first', \n",
    "        'weight':'first', \n",
    "        'games':'sum', \n",
    "        'victories':'sum',\n",
    "        'defeats':'sum', \n",
    "        'ties': 'sum', \n",
    "        'goals':'sum', \n",
    "        'redCards':'sum', \n",
    "        'yellowReds': 'sum', \n",
    "        'yellowCards':'sum',\n",
    "        'skin': most_present\n",
    "\n",
    "\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simplification of the task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem in all its glory is to determine the skin color within the same 5 categories. Because there is some deisagrement between raters. New \"categories\" have been created that lie in between the official ones. Let's look at the distribution of those categories. As we can see below, the categories are skewed to the right (to the \"white\" side of the categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = players['skin'].value_counts(sort=False).sort_index().plot(kind='bar')\n",
    "fig.set_ylabel('Number of players')\n",
    "fig.set_xlabel('Skin \"category\"')\n",
    "fig.set_title('Skin category by players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to reframe the problem to a classification that decide if the playe has light skin or dark skin. Even though we will first try a simple model on the determination of the whole range of skin categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = plt.cm.get_cmap('RdYlBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = players['skin'].value_counts(sort=False).sort_index().plot(kind='bar')\n",
    "fig.set_ylabel('Number of players')\n",
    "fig.set_xlabel('Skin \"category\"')\n",
    "fig.set_title('Skin category by players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models can not work on text features such as the position and such. We will dummy encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = players.copy()\n",
    "X.drop(['skin'], axis=1, inplace=True)\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "y = players.copy()['skin']\n",
    "\n",
    "\n",
    "# Encoding label category\n",
    "cat_encoding = preprocessing.LabelEncoder()\n",
    "y = cat_encoding.fit_transform(y)\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X)\n",
    "\n",
    "# Just create a struct like object\n",
    "dataset = lambda:0\n",
    "dataset.X = X\n",
    "dataset.y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our dataset into train and test. The model will be \"definied/trained\" using the train dataset and the model will be comprared on the classification based on the test set. We don't look at the test set until we do the comparision between different model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, dataset.y, test_size=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define our baseline. We start by selecting randomly the skin color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_random = np.random.randint(2, size=y_test.shape)\n",
    "metrics.f1_score(y_test, y_random, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the score just by selecting the most present skin color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_present = mode(y_train)[0][0] # Model is definied based on train set\n",
    "y_most = np.full(y_test.shape, most_present, dtype=int)\n",
    "metrics.f1_score(y_test, y_most, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to developpe a classifier that will improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRandomForestClassifier():\n",
    "    n_estimators = 30\n",
    "    max_depth = 15\n",
    "    return RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=None, class_weight=\"balanced_subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = getRandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "f1_score = metrics.f1_score(y_test, predictions, average='weighted')\n",
    "\n",
    "print(\"Test set validation score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that just going straight to the goal without much thinking is not going to work. We obtain just a bit of gain compared to selecting the most present class. Let's see where we are on the overfitting side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_train, clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... We overfit _quite_ a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# See: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#see http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = model_selection.learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=\"f1_weighted\")\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = \"Learning Curves\"\n",
    "\n",
    "cv = model_selection.ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "estimator = getRandomForestClassifier()\n",
    "plot_learning_curve(estimator, title, dataset.X, dataset.y, ylim=(0.1, 1.1), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(forest, X, y, columns, n_top=10):\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    \n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1][0:n_top]\n",
    "\n",
    "    #indices = indices\n",
    "    values = list(map(lambda x: importances[x], indices))\n",
    "    stds = list(map(lambda x: std[x], indices))\n",
    "    names = list(map(lambda x: columns[x], indices))\n",
    "    \n",
    "    y = values\n",
    "    N = len(y)\n",
    "    x = range(N)\n",
    "    width = 1/1.5\n",
    "    y_pos = np.arange(len(values))\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(x, y, width, color=\"r\", yerr=stds)\n",
    "    plt.xticks(y_pos+width/2, names, rotation='vertical')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "plot_feature_importance(getRandomForestClassifier(), dataset.X, dataset.y, X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "conf_mat = metrics.confusion_matrix(y_test, predictions)\n",
    "plot_confusion_matrix(conf_mat, classes=[0,1,2,3,4], title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function used to fit kmeans iterations time\n",
    "# Returns the two scores (silhouette + accuracy)\n",
    "def fit_kmeans_on_features(sets, data_in, data_out, iterations = 20):\n",
    "    runs = []\n",
    "    for feature_list in sets:\n",
    "        if feature_list:\n",
    "            results = []\n",
    "            for _ in range(iterations):   \n",
    "                feature_list = list(feature_list)\n",
    "                X = preprocessing.scale(data_in[feature_list])\n",
    "                kmeans = cluster.KMeans(n_clusters=2, init='k-means++', n_init=10)\n",
    "                estimator = kmeans.fit(X)\n",
    "\n",
    "                silhouette_score = metrics.silhouette_score(X, estimator.labels_)    \n",
    "                \n",
    "                # here we need to test both \"skin color\" set attribution. Indeed we can not know in advance if the set\n",
    "                # corresponding to dark skin will be labeled 1 or 0 by kmeans.\n",
    "                #accuracy = max(metrics.f1_score(data_out, estimator.labels_, average='weighted'), metrics.f1_score(~data_out, estimator.labels_, average='weighted'))\n",
    "                accuracy = max(metrics.accuracy_score(data_out, estimator.labels_), metrics.accuracy_score(~data_out, estimator.labels_))\n",
    "\n",
    "                results.append((silhouette_score, accuracy))\n",
    "\n",
    "            runs.append((feature_list, results))\n",
    "            \n",
    "    return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to use unsupervised learning on the aggregated information about the referee for each player. The first step is thus to create a dataframe containing only the information we need. We also binarise the labels to be able to judge the \"accuracy\" of our clustering regarding the skin color.\n",
    "\n",
    "This is how we understand the problem:\n",
    "> We are given referee information about each player (one sample per player) without the skin color attribute. We then need to cluster the samples based on those feature. We then verify if the clustering reflect the skin color. I.E that the two clusters correspond each to one skin color (light / dark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain, combinations\n",
    "\n",
    "# Helper function to generate each combination of a set\n",
    "def get_subsets(s):\n",
    "    return chain(*map(lambda x: combinations(s, x), range(0, len(s)+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following referee information is at our disposal (cf. _data.md_):\n",
    "    \n",
    "- __refNum__ - unique referee ID number (referee name removed for anonymizing purposes)\n",
    "- __refCountry__ - unique referee country ID number (country name removed for anonymizing purposes)\n",
    "- __meanIAT__ - mean implicit bias score (using the race IAT) for referee country, higher values correspond to faster white | good, black | bad associations\n",
    "- __nIAT__ - sample size for race IAT in that particular country\n",
    "- __seIAT__ - standard error for mean estimate of race IAT\n",
    "- __meanExp__ - mean explicit bias score (using a racial thermometer task) for referee country, higher values correspond to greater feelings of warmth toward whites versus blacks\n",
    "- __nExp__ - sample size for explicit bias in that particular country\n",
    "- __seExp__ - standard error for mean estimate of explicit bias measure\n",
    "\n",
    "The data is given as dyad referee-player. Because we need to aggregate the referee information for each player such that we end up with only one sample per player. To start somewhere, we will first take the mean of the following attributes as aggregation technic.\n",
    "\n",
    "- __meanIAT__ - mean implicit bias score (using the race IAT) for referee country, higher values correspond to faster white | good, black | bad associations\n",
    "- __seIAT__ - standard error for mean estimate of race IAT\n",
    "- __meanExp__ - mean explicit bias score (using a racial thermometer task) for referee country, higher values correspond to greater feelings of warmth toward whites versus blacks\n",
    "- __seExp__ - standard error for mean estimate of explicit bias measure\n",
    "\n",
    "Indeed, neither __refNum__ nor __refCountry__ can be aggregated. The sample size will be used afterwards to try to aggregate the values in a more intelligent way than just taking the means of the above values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['meanIAT', 'seIAT', 'seExp', 'meanExp', ]\n",
    "\n",
    "data_clustering = data[features].groupby(level=0).mean()\n",
    "data_clustering_skin = (data['skin'].groupby(level=0).mean() < 0.5).astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to find which feature enable the model to best cluster the data. To judge our model we will use to metrics. \n",
    "    \n",
    "1. __Silhouette Score__: It is used to judge the quality of the clustering. More info can be found [here](https://en.wikipedia.org/wiki/Silhouette_(clustering).\n",
    "    \n",
    "2. __Accurarcy of the clustering regarding the skin color__. This will be used to judge the quality of the clustering regarding a clusering fully based on the skin color. \n",
    "\n",
    "The goal is thus to find the best f1_score and the best silhouette score. As we will see, there is a tradeoff between the two. \n",
    "\n",
    "Testing all combinations of the _features_ is time consuming but in our case were we keep only 4 attributes it is doable. \n",
    "\n",
    "Also, it is important to note that the kmeans is super senitive to the initial conditions. Indeed K-means starts with a random choice of cluster centers. So it may yield different clustering results every time we run the fit function. So to be sure we run multiple times the alorithm and average the result to get a better understanding of the feature set influence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sets = [f for f in get_subsets(features)][1:]\n",
    "sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runs_result = fit_kmeans_on_features(sets, data_clustering, data_clustering_skin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using some feature set, it is possible to obtain a \"good\" _silhouette score_ even though the _accuracy_ stays quiet low. It is most present when we use only one feature to cluster the data. Indeed by using only one feature it is possible to obtain a realy good clustering of the data (good _silhouette score_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = np.mean(np.array([v for f,v in runs_result]), axis=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "sil = ax.plot(means[:,0], label='Silhouette Score')\n",
    "f1 = ax.plot(means[:,1], label='Accuarcy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "labels = sets\n",
    "plt.xticks(range(15), labels, rotation='vertical')\n",
    "ax.set_xlabel('Feature Set')\n",
    "ax.set_ylabel('Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's now try to be smarter about the data at our disposal. Indeed above we aggregated by taking the mean for each feature. But this not take into account the relationship between the attributes. We will now try to combine those to\n",
    "increase the clustering correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "player_group = data.groupby(level=0)\n",
    "player = (data['skin'].groupby(level=0).mean() < 0.5).astype('int8')\n",
    "\n",
    "#Recalculate the mean and std based on the number of samples for each players\n",
    "def custom(g, name):\n",
    "    m = sum(g['mean' + name]*g['n' + name])\n",
    "    s = sum(g['se' + name]*g['n' + name])\n",
    "    n = sum(g['n' + name])\n",
    "    \n",
    "    return pd.Series({\"s\" + name: s/n, \"m\"+name: m/n})\n",
    "\n",
    "iat = player_group.apply(lambda x: custom(x,'IAT'))\n",
    "exp = player_group.apply(lambda x: custom(x,'Exp'))\n",
    "\n",
    "player_info = pd.concat([player,iat,exp], axis=1)\n",
    "\n",
    "data_clustering = player_info\n",
    "data_clustering_skin = (player_info['skin'].groupby(level=0).mean() > 0.5).astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['mIAT', 'sIAT', 'sExp', 'mExp', ]\n",
    "sets = [f for f in get_subsets(features)][1:]\n",
    "runs_result = fit_kmeans_on_features(sets, data_clustering, data_clustering_skin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the recalculation of the features does not improves anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = np.mean(np.array([v for f,v in runs_result]), axis=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "sil = ax.plot(means[:,0], label='Silhouette Score')\n",
    "f1 = ax.plot(means[:,1], label='Accuarcy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "labels = sets\n",
    "plt.xticks(range(15), labels, rotation='vertical')\n",
    "ax.set_xlabel('Feature Set')\n",
    "ax.set_ylabel('Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a question to be asked is are we really doing something useful. Especially is the accuarcy a good method to assess the quality of the clustering regarding the skin color. To do this we will just to the same run as before but instead of looking at the real \"skin\" distribution we will juste decide every body is black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['meanIAT', 'seIAT', 'seExp', 'meanExp', ]\n",
    "\n",
    "data_clustering = data[features].groupby(level=0).mean()\n",
    "data_clustering_skin = (data['skin'].groupby(level=0).mean() < 0.5).astype('bool')\n",
    "sets = [f for f in get_subsets(features)][1:]\n",
    "runs_result = fit_kmeans_on_features(sets, data_clustering, np.ones(len(data_clustering)).astype('bool'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy is way better now. So we can conclude (at least based on our few tries) that there is no easy way to find a clustering that match the skin color.\n",
    "\n",
    "Also is important to note that we only tried kmean but we could also have used knn or any other machine learning algorithm that works on unsupervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = np.mean(np.array([v for f,v in runs_result]), axis=1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "sil = ax.plot(means[:,0], label='Silhouette Score')\n",
    "f1 = ax.plot(means[:,1], label='Accuarcy')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "labels = sets\n",
    "plt.xticks(range(15), labels, rotation='vertical')\n",
    "ax.set_xlabel('Feature Set')\n",
    "ax.set_ylabel('Score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
